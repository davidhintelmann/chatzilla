{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2802d6ed",
   "metadata": {},
   "source": [
    "# Demo chatzilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26980ed0",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "first 7 lines are for relative imports using jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the parent directory to the path\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from chatzilla import zillaping, PromptOllama, ChatOllama\n",
    "from chatzilla.logger import save_history_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c216c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # rename .env.sample to .env\n",
    "ollama_url_ping = os.getenv(\"OLLAMA_URL\") # http://localhost:11434\n",
    "ollama_url_prompt = os.getenv(\"OLLAMA_GEN\") # http://localhost:11434/api/generate\n",
    "ollama_url_chat = os.getenv(\"OLLAMA_CHAT\") # http://localhost:11434/api/chat\n",
    "model = os.getenv(\"DEFAULT_MODEL\") # llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0a92a",
   "metadata": {},
   "source": [
    "## Ping ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b00dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ollama is running'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zillaping(ollama_url_ping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f9208",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Single prompt without any chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd28574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\"You ever notice that anyone driving slower than you is an idiot, and anyone driving faster than you is a maniac? That's not just a coincidence, folks. That's a universal truth.\" - Bill Burr\n"
     ]
    }
   ],
   "source": [
    "prompt = \"tell me a bill burr joke\"\n",
    "joke = PromptOllama(prompt, model, ollama_url_prompt)\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c41d7",
   "metadata": {},
   "source": [
    "## Chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first message received:\n",
      "\n",
      "Here's one:\n",
      "\n",
      "\"You know what's wild? We spend the first year of a child's life teachin' 'em to walk and talk, and the rest of their lives tellin' 'em to shut up and sit down. It's like, what are we even doin'? It's like, I'm not sayin' my kid's stupid or nothin', but I had to explain to him why we can't just take a 30-minute nap on the side of the road every time he gets tired.\"\n",
      "second message received:\n",
      "\n",
      "Warning: this joke is not for everyone. Here's one:\n",
      "\n",
      "\"Have you ever noticed that people who are always telling you to 'follow your heart' and 'be true to yourself'? Those guys are usually the ones who have never had a job, never paid a bill on time, and still live in their mom's basement at 32 years old... (pause) ...and I'm pretty sure they're also the same people who are always trying to get you to sign up for one of those 'multi-level marketing' schemes. You know, the ones where you pay $500 for a vacuum cleaner and then sell vacuum cleaners to your friends? Yeah, good luck with that, buddy... said no one ever.\"\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOllama(ollama_url_chat, model)\n",
    "msg1 = chat.begin(prompt)\n",
    "msg2 = chat.next(\"make the joke edgy\")\n",
    "\n",
    "print(f\"first message received:\\n\\t{msg1}\\n\\n\")\n",
    "print(f\"second message received:\\n\\t{msg2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history_to_json(chat.history())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
